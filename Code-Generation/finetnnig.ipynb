{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize Accelerator\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "dataset_path = \"./data/codegen_finetune_pairs.json\"  # Path to your dataset\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path)\n",
    "\n",
    "# Step 2: Tokenizer and Model\n",
    "model_name = \"Salesforce/codet5-small\"  # Model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"input\"],\n",
    "        text_target=examples[\"output\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "# Add tqdm for tokenization progress\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, desc=\"Tokenizing dataset\")\n",
    "\n",
    "# Step 3: Load Model with Quantization Config\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # Enable 8-bit quantization\n",
    "    llm_int8_threshold=6.0,  # Threshold for weights that remain in float32\n",
    "    llm_int8_skip_modules=[\"lm_head\"],  # Skip quantization for specified modules\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # Automatically map model to available GPUs\n",
    ")\n",
    "\n",
    "# Step 4: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\"],  # Targeting attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Step 5: Training Configuration\n",
    "output_dir = \"./codet5_qlora_finetuned\"\n",
    "eval_results_csv = \"/data/evaluation_results.csv\"  # Path to store evaluation results\n",
    "\n",
    "# Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Save at the end of each epoch\n",
    "    per_device_train_batch_size=2,  # Adjusted for 4GB GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,  # Set default learning rate\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,  # Keep the latest 3 checkpoints\n",
    "    fp16=True,  # Enable mixed precision for better performance\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_bnb_8bit\",  # Optimizer for 8-bit training\n",
    ")\n",
    "\n",
    "# Step 6: Trainer Setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"train\"].select(range(100)),  # Small eval subset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 7: Fine-Tuning with tqdm progress\n",
    "with open(eval_results_csv, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"epoch\", \"eval_loss\", \"eval_runtime\", \"eval_samples_per_second\", \"eval_steps_per_second\"])\n",
    "\n",
    "    with tqdm(total=training_args.num_train_epochs, desc=\"Training epochs\") as pbar:\n",
    "        for epoch in range(training_args.num_train_epochs):\n",
    "            trainer.train()\n",
    "            eval_results = trainer.evaluate()\n",
    "\n",
    "            # Log evaluation results\n",
    "            writer.writerow([\n",
    "                epoch + 1,\n",
    "                eval_results.get(\"eval_loss\"),\n",
    "                eval_results.get(\"eval_runtime\"),\n",
    "                eval_results.get(\"eval_samples_per_second\"),\n",
    "                eval_results.get(\"eval_steps_per_second\"),\n",
    "            ])\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "# Step 8: Save Fine-Tuned Model\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Fine-tuned model saved to {output_dir}\")\n",
    "print(f\"Evaluation results saved to {eval_results_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_dir = \"./codet5_qlora_finetuned\"  # Directory where the fine-tuned model is stored\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "\n",
    "# Input text for code generation\n",
    "input_text = \"Generate code for DBPower C300 HD Camera\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate code using beam search\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=100,           # Maximum length of the generated sequence\n",
    "    num_beams=5,              # Beam search for higher quality outputs\n",
    "    no_repeat_ngram_size=3,   # Prevent repeating the same n-grams\n",
    "    early_stopping=True,      # Stop generation when the model is confident\n",
    "    length_penalty=1.0,       # Penalize longer sequences; adjust as needed\n",
    ")\n",
    "\n",
    "# Decode and display the generated code\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Code:\")\n",
    "print(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMCVEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
