{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter the Exploits data related to Router, Camera, Switch or NVRs then scrap(need to update the filter values for each type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error setting entries per page: Message: \n",
      "\n",
      "Applying filter: Router\n",
      "Scraping page 1 with filter \"Router\" at https://www.exploit-db.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Exploits: 100%|██████████| 120/120 [01:17<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 2 with filter \"Router\" at https://www.exploit-db.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Exploits: 100%|██████████| 120/120 [01:19<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 3 with filter \"Router\" at https://www.exploit-db.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Exploits: 100%|██████████| 102/102 [01:09<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1: Error clicking next page: Message: \n",
      "Stacktrace:\n",
      "#0 0x598e8b618a5a <unknown>\n",
      "#1 0x598e8b1262f0 <unknown>\n",
      "#2 0x598e8b175235 <unknown>\n",
      "#3 0x598e8b175451 <unknown>\n",
      "#4 0x598e8b1badc4 <unknown>\n",
      "#5 0x598e8b199bed <unknown>\n",
      "#6 0x598e8b1b811e <unknown>\n",
      "#7 0x598e8b199963 <unknown>\n",
      "#8 0x598e8b166eec <unknown>\n",
      "#9 0x598e8b16813e <unknown>\n",
      "#10 0x598e8b5e51cf <unknown>\n",
      "#11 0x598e8b5e92dd <unknown>\n",
      "#12 0x598e8b5d36d7 <unknown>\n",
      "#13 0x598e8b5e9a51 <unknown>\n",
      "#14 0x598e8b5bad1e <unknown>\n",
      "#15 0x598e8b607538 <unknown>\n",
      "#16 0x598e8b60773a <unknown>\n",
      "#17 0x598e8b6176cc <unknown>\n",
      "#18 0x7390ce2bbac3 <unknown>\n",
      "\n",
      "Attempt 2: Error clicking next page: Message: \n",
      "Stacktrace:\n",
      "#0 0x598e8b618a5a <unknown>\n",
      "#1 0x598e8b1262f0 <unknown>\n",
      "#2 0x598e8b175235 <unknown>\n",
      "#3 0x598e8b175451 <unknown>\n",
      "#4 0x598e8b1badc4 <unknown>\n",
      "#5 0x598e8b199bed <unknown>\n",
      "#6 0x598e8b1b811e <unknown>\n",
      "#7 0x598e8b199963 <unknown>\n",
      "#8 0x598e8b166eec <unknown>\n",
      "#9 0x598e8b16813e <unknown>\n",
      "#10 0x598e8b5e51cf <unknown>\n",
      "#11 0x598e8b5e92dd <unknown>\n",
      "#12 0x598e8b5d36d7 <unknown>\n",
      "#13 0x598e8b5e9a51 <unknown>\n",
      "#14 0x598e8b5bad1e <unknown>\n",
      "#15 0x598e8b607538 <unknown>\n",
      "#16 0x598e8b60773a <unknown>\n",
      "#17 0x598e8b6176cc <unknown>\n",
      "#18 0x7390ce2bbac3 <unknown>\n",
      "\n",
      "Attempt 3: Error clicking next page: Message: \n",
      "Stacktrace:\n",
      "#0 0x598e8b618a5a <unknown>\n",
      "#1 0x598e8b1262f0 <unknown>\n",
      "#2 0x598e8b175235 <unknown>\n",
      "#3 0x598e8b175451 <unknown>\n",
      "#4 0x598e8b1badc4 <unknown>\n",
      "#5 0x598e8b199bed <unknown>\n",
      "#6 0x598e8b1b811e <unknown>\n",
      "#7 0x598e8b199963 <unknown>\n",
      "#8 0x598e8b166eec <unknown>\n",
      "#9 0x598e8b16813e <unknown>\n",
      "#10 0x598e8b5e51cf <unknown>\n",
      "#11 0x598e8b5e92dd <unknown>\n",
      "#12 0x598e8b5d36d7 <unknown>\n",
      "#13 0x598e8b5e9a51 <unknown>\n",
      "#14 0x598e8b5bad1e <unknown>\n",
      "#15 0x598e8b607538 <unknown>\n",
      "#16 0x598e8b60773a <unknown>\n",
      "#17 0x598e8b6176cc <unknown>\n",
      "#18 0x7390ce2bbac3 <unknown>\n",
      "\n",
      "Attempt 4: Error clicking next page: Message: \n",
      "Stacktrace:\n",
      "#0 0x598e8b618a5a <unknown>\n",
      "#1 0x598e8b1262f0 <unknown>\n",
      "#2 0x598e8b175235 <unknown>\n",
      "#3 0x598e8b175451 <unknown>\n",
      "#4 0x598e8b1badc4 <unknown>\n",
      "#5 0x598e8b199bed <unknown>\n",
      "#6 0x598e8b1b811e <unknown>\n",
      "#7 0x598e8b199963 <unknown>\n",
      "#8 0x598e8b166eec <unknown>\n",
      "#9 0x598e8b16813e <unknown>\n",
      "#10 0x598e8b5e51cf <unknown>\n",
      "#11 0x598e8b5e92dd <unknown>\n",
      "#12 0x598e8b5d36d7 <unknown>\n",
      "#13 0x598e8b5e9a51 <unknown>\n",
      "#14 0x598e8b5bad1e <unknown>\n",
      "#15 0x598e8b607538 <unknown>\n",
      "#16 0x598e8b60773a <unknown>\n",
      "#17 0x598e8b6176cc <unknown>\n",
      "#18 0x7390ce2bbac3 <unknown>\n",
      "\n",
      "Attempt 5: Error clicking next page: Message: \n",
      "Stacktrace:\n",
      "#0 0x598e8b618a5a <unknown>\n",
      "#1 0x598e8b1262f0 <unknown>\n",
      "#2 0x598e8b175235 <unknown>\n",
      "#3 0x598e8b175451 <unknown>\n",
      "#4 0x598e8b1badc4 <unknown>\n",
      "#5 0x598e8b199bed <unknown>\n",
      "#6 0x598e8b1b811e <unknown>\n",
      "#7 0x598e8b199963 <unknown>\n",
      "#8 0x598e8b166eec <unknown>\n",
      "#9 0x598e8b16813e <unknown>\n",
      "#10 0x598e8b5e51cf <unknown>\n",
      "#11 0x598e8b5e92dd <unknown>\n",
      "#12 0x598e8b5d36d7 <unknown>\n",
      "#13 0x598e8b5e9a51 <unknown>\n",
      "#14 0x598e8b5bad1e <unknown>\n",
      "#15 0x598e8b607538 <unknown>\n",
      "#16 0x598e8b60773a <unknown>\n",
      "#17 0x598e8b6176cc <unknown>\n",
      "#18 0x7390ce2bbac3 <unknown>\n",
      "\n",
      "Ending scraping for filter 'Router' after page 3.\n",
      "Scraping completed. Data saved to exploits_filtered.csv.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm  # For progress bar\n",
    "import os\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Function to fetch and parse the page content with retries\n",
    "def fetch_page(retries=5, delay=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Error fetching page content: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return None\n",
    "\n",
    "# Function to scrape the main list of exploits\n",
    "def scrape_exploit_list():\n",
    "    soup = fetch_page()\n",
    "    if not soup:\n",
    "        print(\"Failed to fetch page content.\")\n",
    "        return []\n",
    "\n",
    "    exploit_data = []\n",
    "    table = soup.find('table', {'id': 'exploits-table'})\n",
    "    if table:\n",
    "        rows = table.find_all('tr')[1:]  # Skip header row\n",
    "        if not rows:\n",
    "            print(\"No rows found in table.\")\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                try:\n",
    "                    date = cols[0].text.strip() if cols[0] else 'N/A'\n",
    "                    exploit_link = cols[1].find('a', href=True)['href'] if cols[1].find('a', href=True) else 'N/A'\n",
    "                    description = cols[4].find('a').text.strip() if cols[4].find('a') else 'N/A'\n",
    "                    category = cols[5].find('a').text.strip() if cols[5].find('a') else 'N/A'\n",
    "                    platform = cols[6].find('a').text.strip() if cols[6].find('a') else 'N/A'\n",
    "                    author = cols[7].find('a').text.strip() if cols[7].find('a') else 'N/A'\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                    date, exploit_link, description, category, platform, author = ['N/A'] * 6\n",
    "\n",
    "                if exploit_link and exploit_link != 'N/A':\n",
    "                    exploit_link = f\"https://www.exploit-db.com{exploit_link}\"\n",
    "\n",
    "                exploit_data.append({\n",
    "                    'date': date,\n",
    "                    'exploit_link': exploit_link,\n",
    "                    'description': description,\n",
    "                    'category': category,\n",
    "                    'platform': platform,\n",
    "                    'author': author\n",
    "                })\n",
    "    else:\n",
    "        print(\"Exploit table not found.\")\n",
    "    \n",
    "    return exploit_data\n",
    "\n",
    "# Function to scrape details of each exploit\n",
    "def scrape_exploit_details(exploit_link):\n",
    "    try:\n",
    "        driver.get(exploit_link)\n",
    "        soup = fetch_page()\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        title = soup.find('h1').text.strip() if soup.find('h1') else 'N/A'\n",
    "        platform_detail = soup.find('div', class_='platform').text.strip() if soup.find('div', class_='platform') else 'N/A'\n",
    "        \n",
    "        return {\n",
    "            'exploit_link': exploit_link,\n",
    "            'title': title,\n",
    "            'platform_detail': platform_detail\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {exploit_link}: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Function to wait for page content to fully load\n",
    "def wait_for_page_to_load():\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"table#exploits-table\"))\n",
    "    )\n",
    "    time.sleep(2)  # Optional sleep to ensure full rendering of content\n",
    "\n",
    "# Function to click the next page button with retries\n",
    "def click_next_page(retries=5, delay=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            next_button = WebDriverWait(driver, 20).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//li[@class=\"paginate_button page-item next\"]/a'))\n",
    "            )\n",
    "            next_button.click()\n",
    "            WebDriverWait(driver, 20).until(EC.staleness_of(next_button))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Error clicking next page: {e}\")\n",
    "            time.sleep(delay)\n",
    "    return False\n",
    "\n",
    "# Function to set the search filter\n",
    "def set_search_filter(filter_text):\n",
    "    search_input = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"input[type='search']\"))\n",
    "    )\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(filter_text)\n",
    "    search_input.send_keys(Keys.RETURN)  # Press Enter to submit search\n",
    "\n",
    "# Function to set entries per page\n",
    "def set_entries_per_page():\n",
    "    try:\n",
    "        select_element = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.NAME, 'exploits-table_length'))\n",
    "        )\n",
    "        select = Select(select_element)\n",
    "        select.select_by_value('120')\n",
    "        WebDriverWait(driver, 10).until(EC.staleness_of(select_element))\n",
    "        print(\"Set entries per page to 120.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting entries per page: {e}\")\n",
    "\n",
    "# Function to check if CSV file exists and write the header only once\n",
    "def write_csv_header(file_path, fieldnames):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            print(\"CSV header written.\")\n",
    "\n",
    "# Main scraping loop with individual filter\n",
    "def main():\n",
    "    base_url = 'https://www.exploit-db.com'\n",
    "    driver.get(base_url)\n",
    "\n",
    "    set_entries_per_page()\n",
    "\n",
    "    # List of filters to be applied one by one\n",
    "    filters = ['Router']  # Modify as needed\n",
    "\n",
    "    file_path = 'exploits_filtered.csv'\n",
    "    fieldnames = ['date', 'exploit_link', 'description', 'category', 'platform', 'author', 'title', 'platform_detail']\n",
    "    \n",
    "    # Write header if the CSV file doesn't exist\n",
    "    write_csv_header(file_path, fieldnames)\n",
    "\n",
    "    with open(file_path, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "\n",
    "        for filter_text in filters:\n",
    "            print(f\"Applying filter: {filter_text}\")\n",
    "            set_search_filter(filter_text)\n",
    "\n",
    "            page_number = 1\n",
    "            while True:\n",
    "                wait_for_page_to_load()  # Wait for content to load\n",
    "                print(f'Scraping page {page_number} with filter \"{filter_text}\" at {driver.current_url}')\n",
    "                exploit_links = scrape_exploit_list()\n",
    "\n",
    "                if not exploit_links:\n",
    "                    print(f\"No exploits found on page {page_number} with filter '{filter_text}'.\")\n",
    "                    break\n",
    "\n",
    "                valid_links = [link['exploit_link'] for link in exploit_links if link['exploit_link'] and link['exploit_link'] != 'N/A']\n",
    "\n",
    "                if not valid_links:\n",
    "                    print(f\"No valid exploit links found on page {page_number} with filter '{filter_text}'.\")\n",
    "                    break\n",
    "\n",
    "                with ThreadPoolExecutor(max_workers=3) as executor:  # Reduced workers to avoid overloading\n",
    "                    results = list(tqdm(executor.map(scrape_exploit_details, valid_links), total=len(valid_links), desc=\"Scraping Exploits\"))\n",
    "\n",
    "                for exploit in exploit_links:\n",
    "                    details = next((item for item in results if item['exploit_link'] == exploit['exploit_link']), {})\n",
    "                    if details:\n",
    "                        exploit.update(details)\n",
    "                        writer.writerow(exploit)\n",
    "\n",
    "                if not click_next_page():\n",
    "                    print(f\"Ending scraping for filter '{filter_text}' after page {page_number}.\")\n",
    "                    break\n",
    "\n",
    "                page_number += 1\n",
    "\n",
    "    print('Scraping completed. Data saved to exploits_filtered.csv.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the and load the exploits codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to ./exploits_filtered_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the CSV file path\n",
    "file_path = './data/exploits_filtered.csv'\n",
    "\n",
    "# Specify the local directory where files are stored\n",
    "local_dir = './downloads'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to update exploit_link to local paths\n",
    "def update_to_local_paths(row):\n",
    "    file_name = os.path.basename(row['exploit_link'])\n",
    "    local_path = os.path.join(local_dir, file_name)\n",
    "    return local_path\n",
    "\n",
    "# Update the exploit_link column\n",
    "data['exploit_link'] = data.apply(update_to_local_paths, axis=1)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "updated_csv_path = '../data/exploits_filtered_updated.csv'\n",
    "data.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved to {updated_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# update the the file data into the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extracted and saved to ./extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Define the download folder path (you can change this to your specific folder)\n",
    "download_folder = './downloads/'\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv = './data/extracted_data.csv'\n",
    "\n",
    "# Example data frame (replace this with your actual data)\n",
    "data = pd.read_csv('./data/exploits_filtered_updated.csv')\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "columns = ['date', 'exploit_link', 'description', 'category', 'platform', 'author', 'title']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Function to extract files based on the exploit_link\n",
    "def extract_files_from_links(df, download_folder, output_csv):\n",
    "    extracted_data = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        file_link = row['exploit_link']\n",
    "        file_path = os.path.join(download_folder, file_link.split('/')[-1])  # Get file name from the link\n",
    "\n",
    "        if os.path.exists(file_path):  # Check if file exists\n",
    "            # Assuming the file is a text file or any readable file\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_data = file.read()\n",
    "\n",
    "            # You can modify this logic if you want to process file contents\n",
    "            extracted_data.append(row.tolist() + [file_data])  # Add file content to the row data\n",
    "        else:\n",
    "            extracted_data.append(row.tolist() + ['File Not Found'])  # If the file is not found\n",
    "\n",
    "    # Create a new DataFrame for extracted data\n",
    "    extracted_columns = columns + ['file_data']\n",
    "    extracted_df = pd.DataFrame(extracted_data, columns=extracted_columns)\n",
    "\n",
    "    # Save to CSV\n",
    "    extracted_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Data extracted and saved to {output_csv}\")\n",
    "\n",
    "# Run the extraction function\n",
    "extract_files_from_links(df, download_folder, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMCVEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
