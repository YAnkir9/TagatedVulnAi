{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data (only code) has been saved to ./data/processed_extracted_code_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16457/3813204579.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  cleaned_data['date'] = pd.to_datetime(cleaned_data['date'], errors='coerce')\n",
      "/tmp/ipykernel_16457/3813204579.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['date'] = pd.to_datetime(cleaned_data['date'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './data/extracted_data.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: General Cleaning\n",
    "# Remove duplicate rows based on relevant columns\n",
    "cleaned_data = data.drop_duplicates(subset=['exploit_link', 'description'])\n",
    "\n",
    "# Standardize the date format\n",
    "cleaned_data['date'] = pd.to_datetime(cleaned_data['date'], errors='coerce')\n",
    "\n",
    "# Remove unnecessary columns (e.g., 'title')\n",
    "cleaned_data = cleaned_data.drop(columns=['title'])\n",
    "\n",
    "# Drop rows with missing or invalid 'file_data'\n",
    "cleaned_data = cleaned_data.dropna(subset=['file_data'])\n",
    "\n",
    "# Step 2: Clean and Normalize the `file_data` Column\n",
    "def clean_file_data(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    try:\n",
    "        # Attempt to parse the value as JSON\n",
    "        parsed = json.loads(value)\n",
    "        if isinstance(parsed, dict):\n",
    "            return parsed\n",
    "        else:\n",
    "            return value\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Return None for invalid JSON, else keep the original value\n",
    "        return value if isinstance(value, str) else None\n",
    "\n",
    "# Apply the cleaning function to the 'file_data' column\n",
    "cleaned_data['file_data'] = cleaned_data['file_data'].apply(clean_file_data)\n",
    "\n",
    "# Step 3: Remove special characters from the `file_data` field based on specific logic\n",
    "def clean_special_characters_from_file_data(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    lines = text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        # If the line has only special characters and does not represent a comment, remove it\n",
    "        if re.match(r'^\\s*[!@#$%^&*()_+=\\[\\]{}|;:\\'\",.<>?/\\\\`~\\-]+\\s*$', line):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "# Apply the cleaning function to the `file_data` field\n",
    "cleaned_data['file_data'] = cleaned_data['file_data'].apply(clean_special_characters_from_file_data)\n",
    "\n",
    "# Step 4: Further Processing - Split Description and File Data\n",
    "# Define a function to split the description into 'device_detail' and 'exploited_component'\n",
    "def split_description(description):\n",
    "    if ' - ' in description:\n",
    "        parts = description.split(' - ', 1)\n",
    "        return parts[0], parts[1]\n",
    "    return description, ''  # If no separator is found, return the whole description as device_detail\n",
    "\n",
    "# Apply the function to create new columns\n",
    "cleaned_data['device_detail'], cleaned_data['exploited_component'] = zip(*cleaned_data['description'].apply(split_description))\n",
    "\n",
    "# Step 5: Define a function to separate file_data into 'details' and 'code' (moving sources to 'details')\n",
    "def split_file_data(file_data):\n",
    "    if not isinstance(file_data, str):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    lines = file_data.split('\\n')\n",
    "    details = []\n",
    "    code = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Skip lines that appear to be URLs, comments, or descriptions\n",
    "        if re.match(r'^\\s*(#|\\/\\/|\\/\\*|https?://|www\\.|\\.com|\\.org)', stripped_line):\n",
    "            details.append(stripped_line)\n",
    "            continue\n",
    "        \n",
    "        # Skip lines that describe the code or refer to sources\n",
    "        elif re.match(r'^\\s*(description:|desc:|summary:|source:|src:|sources:|source path:)', stripped_line, re.IGNORECASE):\n",
    "            details.append(stripped_line)\n",
    "            continue\n",
    "        \n",
    "        # Skip lines that are just commands or script metadata, such as echo or curl\n",
    "        elif re.match(r'^\\s*(echo|curl|wget|sh|bash|python|perl|ruby|chmod|chown|mkdir|rm|cat|ls|tar|grep|awk|sed|lsblk|cp|mv)', stripped_line):\n",
    "            code.append(stripped_line)\n",
    "            continue\n",
    "\n",
    "        # Treat any line that looks like a code snippet\n",
    "        elif re.match(r'^\\s*(if|for|while|function|return|class|def|try|catch|import|from|exec|require|open|read|write)', stripped_line):\n",
    "            code.append(stripped_line)\n",
    "            continue\n",
    "\n",
    "        # Treat remaining lines as code\n",
    "        elif stripped_line and not stripped_line.startswith(('description:', 'desc:', 'summary:', 'source:', 'src:', 'sources:', 'source path:')):\n",
    "            code.append(stripped_line)\n",
    "    \n",
    "    # Join the details and code separately\n",
    "    details = \"\\n\".join(details).strip()\n",
    "    code = \"\\n\".join(code).strip()\n",
    "    return details, code\n",
    "\n",
    "# Apply the function to create new 'details' and 'code' columns\n",
    "cleaned_data['details'], cleaned_data['code'] = zip(*cleaned_data['file_data'].apply(split_file_data))\n",
    "\n",
    "# Step 6: Drop unnecessary columns after splitting\n",
    "cleaned_data = cleaned_data.drop(columns=['exploit_link', 'description', 'file_data'])\n",
    "\n",
    "# Step 7: Save the cleaned and processed dataset to a new CSV file\n",
    "output_path = './data/processed_extracted_code_data.csv'  # Path for saving only the extracted code\n",
    "cleaned_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Processed data (only code) has been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMCVEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
