{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuned T5 with Peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import ast\n",
    "\n",
    "# Enable cuDNN benchmark for GPU optimization\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Clear memory\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "class CVEDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_length=128, max_output_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query = self.data[idx]['input']\n",
    "        answer_data = self.data[idx].get('output', {}).get('answer', '')\n",
    "\n",
    "        if isinstance(answer_data, list):\n",
    "            answer = \"; \".join([str(item) for item in answer_data])\n",
    "        elif isinstance(answer_data, dict):\n",
    "            answer = str(answer_data)\n",
    "        else:\n",
    "            answer = str(answer_data)\n",
    "\n",
    "        input_encodings = self.tokenizer(query, truncation=True, padding=\"max_length\", \n",
    "                                         max_length=self.max_input_length, return_tensors=\"pt\")\n",
    "        output_encodings = self.tokenizer(answer, truncation=True, padding=\"max_length\", \n",
    "                                          max_length=self.max_output_length, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = input_encodings['input_ids'].squeeze()\n",
    "        attention_mask = input_encodings['attention_mask'].squeeze()\n",
    "        labels = output_encodings['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "def train_with_peft(model, train_loader, val_loader, optimizer, scaler, device, accumulation_steps=4, epochs=3):\n",
    "    model.train()\n",
    "    epoch_results = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for step, batch in enumerate(tqdm(train_loader, desc=\"Training\", ncols=100)):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        epoch_results.append({\"epoch\": epoch + 1, \"avg_loss\": avg_epoch_loss})\n",
    "\n",
    "        # Validate the model\n",
    "        validate_loss = validate_model(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch + 1} Validation Loss: {validate_loss:.4f}\")\n",
    "        epoch_results[-1][\"val_loss\"] = validate_loss\n",
    "\n",
    "        clear_memory()\n",
    "\n",
    "    return epoch_results\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\", ncols=100):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(val_loader)\n",
    "\n",
    "def main():\n",
    "    # Load existing prompts\n",
    "    prompts_data = pd.read_csv('./data/generated_input_output_pairs.csv')\n",
    "    prompts_data['output'] = prompts_data['output'].apply(ast.literal_eval)  # Convert stringified dicts back to dictionaries\n",
    "    dynamic_prompts = prompts_data.to_dict(orient='records')\n",
    "\n",
    "    # Prepare tokenizer and dataset\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    dataset = CVEDataset(dynamic_prompts, tokenizer)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # DataLoader\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, collate_fn=data_collator, shuffle=True, pin_memory=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=data_collator, pin_memory=True, num_workers=8)\n",
    "\n",
    "    # Model and PEFT setup\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training and validation with PEFT\n",
    "    epoch_results = train_with_peft(model, train_loader, val_loader, optimizer, scaler, device, accumulation_steps=4, epochs=3)\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "    # Save validation results to CSV\n",
    "    pd.DataFrame(epoch_results).to_csv('./validation_results.csv', index=False)\n",
    "    print(\"Validation results saved to './validation_results.csv'\")\n",
    "\n",
    "    # Save PEFT model and tokenizer\n",
    "    model.save_pretrained('./fine_tuned_t5_peft')\n",
    "    tokenizer.save_pretrained('./fine_tuned_t5_peft')\n",
    "    print(\"PEFT model and tokenizer saved to './fine_tuned_t5_peft'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
