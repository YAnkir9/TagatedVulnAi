{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-top sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = './fine_tuned_t5_peft'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Function to generate response from model\n",
    "def generate_response(input_text, max_length=128, temperature=0.7, top_k=50):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Chatbot interface\n",
    "def chatbot():\n",
    "    print(\"Chat with your fine-tuned model! Type 'exit' to end the chat.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chat ended. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add a prompt prefix for inference\n",
    "        model_input = f\"{user_input}\"\n",
    "        print(model_input)\n",
    "        response = generate_response(model_input)\n",
    "        print(f\"Model: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "MODEL_PATH = './fine_tuned_t5_peft'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Function to generate response from model\n",
    "def generate_response(input_text, max_length=128, temperature=0.7, top_k=50):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return decoded_output\n",
    "\n",
    "# Chatbot interface\n",
    "def chatbot():\n",
    "    print(\"Chat with your fine-tuned model! Type 'exit' to end the chat.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chat ended. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Add a prompt prefix for inference\n",
    "        model_input = f\"{user_input}\"\n",
    "        print(model_input)\n",
    "        response = generate_response(model_input)\n",
    "        print(f\"Model: {response}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot inferencing with Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# # Load the fine-tuned model and tokenizer\n",
    "# MODEL_PATH = './fine_tuned_t5_small'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # Load model and tokenizer\n",
    "# model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
    "# tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# def generate_zero_shot_response(input_text, max_length=512, num_beams=5):\n",
    "#     # Tokenize input text\n",
    "#     input_ids = tokenizer(input_text, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "    \n",
    "#     # Generate output with beam search (zero-shot)\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=max_length,\n",
    "#         num_beams=num_beams,\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "    \n",
    "#     # Decode the generated text\n",
    "#     decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return decoded_output\n",
    "\n",
    "# def chatbot_zero_shot():\n",
    "#     print(\"Zero-shot Chatbot: Type 'exit' to end the chat.\\n\")\n",
    "#     while True:\n",
    "#         user_input = input(\"You: \")\n",
    "#         if user_input.lower() == 'exit':\n",
    "#             print(\"Chat ended. Goodbye!\")\n",
    "#             break\n",
    "        \n",
    "#         model_input = f\"{user_input}\"  # Zero-shot: no example needed\n",
    "#         response = generate_zero_shot_response(model_input)\n",
    "#         print(f\"Model: {response}\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     chatbot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# import faiss\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "# # Load FAISS index\n",
    "# faiss_index_path = './data/faiss_index.index'\n",
    "# filtered_data_csv = './data/filtered_cve_data2.csv'\n",
    "\n",
    "# # Load FAISS index\n",
    "# index = faiss.read_index(faiss_index_path)\n",
    "\n",
    "# # Load document dataset\n",
    "# filtered_df = pd.read_csv(filtered_data_csv)\n",
    "# # documents = filtered_df['Description'].tolist()\n",
    "\n",
    "# # Model setup\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# sentence_model = SentenceTransformer('all-MiniLM-L6-v2')  # Replace with your embedding model\n",
    "# tokenizer = T5Tokenizer.from_pretrained('./flan_t5_finetuned')\n",
    "# model = T5ForConditionalGeneration.from_pretrained('./flan_t5_finetuned').to(device)\n",
    "\n",
    "# def get_relevant_documents(query_vec, k=5):\n",
    "#     distances, indices = index.search(query_vec, k)\n",
    "#     if len(indices) == 0 or len(indices[0]) == 0:\n",
    "#         return []\n",
    "#     retrieved_docs = [filtered_df[i] for i in indices[0] if i < len(filtered_df)]\n",
    "#     return retrieved_docs\n",
    "\n",
    "# def generate_response_with_retrieved_context(input_text, max_length=512, num_beams=5, top_k=50, do_sample=True):\n",
    "#     query_vec = sentence_model.encode([input_text], convert_to_numpy=True).astype('float32')\n",
    "#     retrieved_docs = get_relevant_documents(query_vec)\n",
    "#     if not retrieved_docs:\n",
    "#         return \"I couldn't find relevant information. Please try rephrasing your question.\"\n",
    "\n",
    "#     context = \" \".join(retrieved_docs)\n",
    "#     model_input = f\"Question: {input_text} Context: {context}\"\n",
    "#     input_ids = tokenizer(model_input, return_tensors='pt', padding=True, truncation=True).input_ids.to(device)\n",
    "\n",
    "#     outputs = model.generate(\n",
    "#         input_ids,\n",
    "#         max_length=max_length,\n",
    "#         num_beams=num_beams,\n",
    "#         top_k=top_k,\n",
    "#         do_sample=do_sample,\n",
    "#         early_stopping=True,\n",
    "#         num_return_sequences=1\n",
    "#     )\n",
    "\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# def chatbot():\n",
    "#     print(\"RAG Chatbot: Type 'exit' to end the chat.\\n\")\n",
    "#     while True:\n",
    "#         user_input = input(\"You: \")\n",
    "#         if user_input.lower() == 'exit':\n",
    "#             print(\"Chat ended. Goodbye!\")\n",
    "#             break\n",
    "#         response = generate_response_with_retrieved_context(user_input)\n",
    "#         print(f\"Model: {response}\\n\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMCVEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
