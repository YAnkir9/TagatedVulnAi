{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, \n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback, get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "# Enable dynamic GPU memory allocation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('./data/generated_input_output_pairs.csv')\n",
    "\n",
    "# Normalize and preprocess text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['input'] = df['input'].apply(normalize_text)\n",
    "df['output'] = df['output'].apply(normalize_text)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# Load pre-trained tokenizer and model\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Enable gradient checkpointing for memory optimization\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Freeze certain model layers to save memory and speed up training\n",
    "for param in model.encoder.block[:3].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    bias=\"none\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    outputs = tokenizer(examples['output'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=8)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True, num_proc=8)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Accelerator\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, train_dataset, val_dataset, data_collator = accelerator.prepare(\n",
    "    model, train_dataset, val_dataset, data_collator\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = (len(train_dataset) // 16) * 3  # Adjust for batch size and epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    logging_steps=200,\n",
    "    max_grad_norm=1.5,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=3e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset.shuffle(seed=42).select(range(100)),\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=2),\n",
    "        TensorBoardCallback()\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Initial Learning Rate: {training_args.learning_rate}\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {results}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame({\n",
    "    \"epoch\": [trainer.state.epoch],\n",
    "    \"train_loss\": [trainer.state.best_metric],\n",
    "    \"eval_loss\": [results.get(\"eval_loss\")],\n",
    "})\n",
    "results_df.to_csv('./results/model_training_evaluation_results.csv', index=False)\n",
    "\n",
    "print(\"Training complete. Results saved to './results/model_training_evaluation_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetunnig with Rogue Score\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "# from accelerate import Accelerator\n",
    "# import evaluate\n",
    "\n",
    "# # Enable dynamic GPU memory allocation\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "# # Load the dataset\n",
    "# df = pd.read_csv('./data/generated_input_output_pairs.csv')\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# # Split dataset into train and validation sets\n",
    "# train_dataset, val_dataset = dataset.train_test_split(test_size=0.1, seed=42).values()\n",
    "\n",
    "# # Load pre-trained T5 tokenizer and model\n",
    "# model_name = \"google/flan-t5-small\"\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name, use_fast=True,legacy=False)  # Use non-legacy tokenizer behavior\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# # Apply QLoRA using PEFT\n",
    "# lora_config = LoraConfig(\n",
    "#     task_type=TaskType.SEQ_2_SEQ_LM,  # Task type for T5\n",
    "#     r=8,  # Low-rank matrix dimension\n",
    "#     lora_alpha=32,  # Scaling factor\n",
    "#     lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "#     target_modules=[\"q\", \"v\"],  # Target attention weights for adaptation\n",
    "#     bias=\"none\",\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "\n",
    "# # Tokenization function with increased max_length\n",
    "# def tokenize_function(examples):\n",
    "#     inputs = tokenizer(\n",
    "#         examples['input'], padding=\"max_length\", truncation=True, max_length=128  # Increase max_length\n",
    "#     )\n",
    "#     outputs = tokenizer(\n",
    "#         examples['output'], padding=\"max_length\", truncation=True, max_length=128  # Increase max_length\n",
    "#     )\n",
    "#     inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "#     return inputs\n",
    "\n",
    "# # Tokenize datasets\n",
    "# train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "# val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# # Define DataCollator for efficient padding\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# # Load evaluation metrics\n",
    "# bleu_metric = evaluate.load(\"bleu\")\n",
    "# rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# # Compute metrics\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     if isinstance(predictions, tuple):\n",
    "#         predictions = predictions[0]\n",
    "#     predictions = predictions.argmax(axis=-1) if predictions.ndim > 2 else predictions\n",
    "#     labels = labels.tolist()\n",
    "#     for i in range(len(labels)):\n",
    "#         labels[i] = [label if label != -100 else tokenizer.pad_token_id for label in labels[i]]\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "#     bleu = bleu_metric.compute(predictions=decoded_preds, references=[[lbl] for lbl in decoded_labels])\n",
    "#     rouge = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     rouge_l_score = rouge.get(\"rouge-l\", {}).get(\"mid\", {}).get(\"fmeasure\", 0)\n",
    "#     return {\n",
    "#         \"bleu\": bleu[\"bleu\"],\n",
    "#         \"rouge\": rouge_l_score,\n",
    "#     }\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = Seq2SeqTrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     # num_train_epochs=,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     gradient_accumulation_steps=16,\n",
    "#     warmup_steps=10,\n",
    "#     logging_dir='./logs',\n",
    "#     logging_steps=100,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     report_to=\"tensorboard\",\n",
    "#     fp16=True,  # Use mixed precision for faster training\n",
    "#     dataloader_pin_memory=True,\n",
    "#     dataloader_num_workers=4,\n",
    "#     overwrite_output_dir=True,\n",
    "# )\n",
    "\n",
    "# # Setup Accelerator\n",
    "# accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "# # Prepare model, datasets, and data collator for Accelerator\n",
    "# model, train_dataset, val_dataset, data_collator = accelerator.prepare(\n",
    "#     model, train_dataset, val_dataset, data_collator\n",
    "# )\n",
    "\n",
    "# # Define Trainer with early stopping callback\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=data_collator,\n",
    "#     callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the model and tokenizer\n",
    "# model.save_pretrained(\"./flanfine_tuned_model\")\n",
    "# tokenizer.save_pretrained(\"./flanfine_tuned_model\")\n",
    "\n",
    "# # Evaluate and save results\n",
    "# results = trainer.evaluate()\n",
    "# results_df = pd.DataFrame({\n",
    "#     \"epoch\": [trainer.state.epoch],\n",
    "#     \"train_loss\": [trainer.state.best_metric],\n",
    "#     \"eval_loss\": [results.get(\"eval_loss\")],\n",
    "#     \"bleu\": [results.get(\"eval_bleu\", float(\"nan\"))],\n",
    "#     \"rouge\": [results.get(\"eval_rouge\", float(\"nan\"))],\n",
    "# })\n",
    "# results_df.to_csv('./results/model_training_evaluation_results.csv', index=False)\n",
    "\n",
    "# print(\"Training and evaluation complete. Results saved to './results/model_training_evaluation_results.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMCVEs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
